{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d316d7",
   "metadata": {},
   "source": [
    "## Spark Dataframe Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da7e29",
   "metadata": {},
   "source": [
    "### Importing Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf859253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helps to connect to spark and python by just passing the path of spark directory\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.1.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241af95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9fda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Session helps to work with Spark Dataframes\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40662fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating object by building the app\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884678a",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a25354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f7075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71050f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Columns names and their datatypes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30363c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49de392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statistical summary of dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c013fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac0fee",
   "metadata": {},
   "source": [
    "**Sometimes we need to clarify the schema of the data. Spark is very good in inferring the Schema but this is done to show that there is a way to change it. Will not be useful much!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa254fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing datatypes\n",
    "from pyspark.sql.types import (StructField,StructType,\n",
    "                              StringType,IntegerType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24acc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining datatype for schema\n",
    "\n",
    "#StructField takes 3 parameters :- column name,Data type name, Nullable or not (can be null or not)\n",
    "data_schema = [StructField('age',IntegerType(),True),\n",
    "               StructField('name',StringType(),True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd4a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Providing defined schema in StructType\n",
    "final_struc = StructType(data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91251d",
   "metadata": {},
   "source": [
    "#### Reading JSON file with the defined Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d0f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people.json', schema = final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c74d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PrintSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4c6b4",
   "metadata": {},
   "source": [
    "#### Fetching column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "535f842b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "039e4840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd01e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using select we can fetch columns\n",
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2bb08c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a73486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Accessing multiple columns\n",
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d407251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing single row\n",
    "df.head(3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "888cabde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating new column\n",
    "new_df = df.withColumn('double_age',df['age']*2)\n",
    "new_df.show()\n",
    "\n",
    "#This method takes the name of new column and the data to put in it (in column type format)\n",
    "#This is not saved in the original dataframe and thus we need to save it in a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f67a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4576dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|my_new_age|   name|\n",
      "+----------+-------+\n",
      "|      null|Michael|\n",
      "|        30|   Andy|\n",
      "|        19| Justin|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming columns\n",
    "df.withColumnRenamed('age','my_new_age').show()\n",
    "\n",
    "#This is not saved in the original dataframe and thus we need to save it in a new variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee9651",
   "metadata": {},
   "source": [
    "#### Working with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcac73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a temporary SQL view with table name 'people'\n",
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a877b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directly passing SQL query\n",
    "results = spark.sql('SELECT * from people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b563c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e3ccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = spark.sql('select * from people where age = 30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897156f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf4167",
   "metadata": {},
   "source": [
    "## Spark Dataframe basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f4783f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv files\n",
    "df = spark.read.csv('apple_stock.csv',inferSchema=True,header=True)\n",
    "\n",
    "#inferSchema lets you automatically find the schema of csv data (does not have option in json)\n",
    "#header tells that the first row is the column names of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0c6d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b69fcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Showing just 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c679156c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "|2010-02-02|        195.909998|        196.319994|193.37999299999998|        195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04|        196.730003|        198.370001|        191.570005|        192.050003|189413000|         24.881912|\n",
      "|2010-02-05|192.63000300000002|             196.0|        190.850002|        195.460001|212576700|25.323710000000002|\n",
      "|2010-02-08|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n",
      "|2010-02-09|        196.419996|        197.499994|        194.749998|196.19000400000002|158221700|         25.418289|\n",
      "|2010-02-10|        195.889997|             196.6|            194.26|195.12000700000002| 92590400|          25.27966|\n",
      "|2010-02-11|        194.880001|        199.750006|194.05999599999998|        198.669994|137586400|         25.739595|\n",
      "|2010-02-23|        199.999998|        201.330002|        195.709993|        197.059998|143773700|         25.531005|\n",
      "|2014-06-09|         92.699997|         93.879997|             91.75|         93.699997| 75415000|         88.906324|\n",
      "|2014-06-10|         94.730003|         95.050003|             93.57|             94.25| 62777000|         89.428189|\n",
      "|2014-06-11|         94.129997|         94.760002|         93.470001|         93.860001| 45681000|         89.058142|\n",
      "|2014-06-12|         94.040001|         94.120003|         91.900002|         92.290001| 54749000|         87.568463|\n",
      "|2014-06-13|         92.199997|         92.440002|         90.879997|         91.279999| 54525000|         86.610132|\n",
      "|2014-06-16|         91.510002|             92.75|         91.449997|         92.199997| 35561000|         87.483064|\n",
      "|2014-06-17|         92.309998|         92.699997|         91.800003| 92.08000200000001| 29726000| 87.36920699999999|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting required data using sql in backend\n",
    "df.filter('Close < 200').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66fb725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|              Open|             Close|\n",
      "+------------------+------------------+\n",
      "|206.78000600000001|            197.75|\n",
      "|        204.930004|        199.289995|\n",
      "|        201.079996|        192.060003|\n",
      "|192.36999699999998|        194.729998|\n",
      "|        195.909998|        195.859997|\n",
      "|        195.169994|        199.229994|\n",
      "|        196.730003|        192.050003|\n",
      "|192.63000300000002|        195.460001|\n",
      "|        195.690006|194.11999699999998|\n",
      "|        196.419996|196.19000400000002|\n",
      "|        195.889997|195.12000700000002|\n",
      "|        194.880001|        198.669994|\n",
      "|        199.999998|        197.059998|\n",
      "|         92.699997|         93.699997|\n",
      "|         94.730003|             94.25|\n",
      "|         94.129997|         93.860001|\n",
      "|         94.040001|         92.290001|\n",
      "|         92.199997|         91.279999|\n",
      "|         91.510002|         92.199997|\n",
      "|         92.309998| 92.08000200000001|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Just retrieving 'Open' and 'Close' column from filtered output\n",
    "df.filter('Close < 200').select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e3e5aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "|2010-02-02|        195.909998|        196.319994|193.37999299999998|        195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04|        196.730003|        198.370001|        191.570005|        192.050003|189413000|         24.881912|\n",
      "|2010-02-05|192.63000300000002|             196.0|        190.850002|        195.460001|212576700|25.323710000000002|\n",
      "|2010-02-08|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n",
      "|2010-02-09|        196.419996|        197.499994|        194.749998|196.19000400000002|158221700|         25.418289|\n",
      "|2010-02-10|        195.889997|             196.6|            194.26|195.12000700000002| 92590400|          25.27966|\n",
      "|2010-02-11|        194.880001|        199.750006|194.05999599999998|        198.669994|137586400|         25.739595|\n",
      "|2010-02-23|        199.999998|        201.330002|        195.709993|        197.059998|143773700|         25.531005|\n",
      "|2014-06-09|         92.699997|         93.879997|             91.75|         93.699997| 75415000|         88.906324|\n",
      "|2014-06-10|         94.730003|         95.050003|             93.57|             94.25| 62777000|         89.428189|\n",
      "|2014-06-11|         94.129997|         94.760002|         93.470001|         93.860001| 45681000|         89.058142|\n",
      "|2014-06-12|         94.040001|         94.120003|         91.900002|         92.290001| 54749000|         87.568463|\n",
      "|2014-06-13|         92.199997|         92.440002|         90.879997|         91.279999| 54525000|         86.610132|\n",
      "|2014-06-16|         91.510002|             92.75|         91.449997|         92.199997| 35561000|         87.483064|\n",
      "|2014-06-17|         92.309998|         92.699997|         91.800003| 92.08000200000001| 29726000| 87.36920699999999|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#another method\n",
    "df.filter(df['Close'] < 200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0e32aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|      Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+----------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Multiple condition is single line\n",
    "df.filter( (df['Close'] < 200) & (df['Open'] > 200) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "873a574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the output from the result obtained\n",
    "result = df.filter(df['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a79d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving first row in variable (returns a dictionary)\n",
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdd8c34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2010-01-22', Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a22cab08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': '2010-01-22',\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52b2290e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieving value using key\n",
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea7e5a",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba69730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales_info.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c3415e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87f349ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88ae97d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f5e087f2370>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd7366d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average of the sales by grouping company\n",
    "df.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cea4589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum of the sales by grouping company\n",
    "df.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca858f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max sale by grouping company\n",
    "df.groupBy('Company').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "986a9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of sales by grouping company\n",
    "df.groupBy('Company').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43e29034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sum of all the sales rows (same will be finding max,min among all rows)\n",
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6abd01",
   "metadata": {},
   "source": [
    "### Importing functions from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96be5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct,avg,stddev,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc3bd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    Average Sales|\n",
      "+-----------------+\n",
      "|360.5833333333333|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avg sales\n",
    "df.select(avg('Sales').alias('Average Sales')).show()\n",
    "\n",
    "#Alias helps in renaming the column name (eg- 'Average Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42a3d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of sales\n",
    "sales_std = df.select(stddev('Sales').alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d50ee3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Formatting the above number to two numbers\n",
    "sales_std.select(format_number('std',2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f4d7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ordering of numbers \n",
    "df.orderBy('Sales').show()\n",
    "\n",
    "#Ascending Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef327835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ordering of numbers \n",
    "df.orderBy(df['Sales'].desc()).show()\n",
    "\n",
    "#Descending Order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb6eb5",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e6c3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('ContainsNull.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c9d1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5195c3b",
   "metadata": {},
   "source": [
    "#### Dropping Empty Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89446495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "738631bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing those records which dont have two non-null values\n",
    "df.na.drop(thresh=2).show()\n",
    "\n",
    "#Like here, We have emp1,John as two non-null value so this record is not removed whereas the record which is removed was not having two non null records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac972f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove Null record only from Sales column\n",
    "df.na.drop(subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463289b",
   "metadata": {},
   "source": [
    "#### Filling Empty Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07b01b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "defd3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|Fill Value| null|\n",
      "|emp3|Fill Value|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fills value in the string field 'Name'\n",
    "df.na.fill('Fill Value',['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb10c53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  4.0|\n",
      "|emp2| null|  4.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fills number in the interger field 'Sales'\n",
    "df.na.fill(4,subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d934cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling with mean\n",
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d763f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect helps to collect the value instead of just showing it\n",
    "mean_val = df.select(mean(df['Sales'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b59e2503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Sales)=400.5)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "363a4045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetching the value\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb276d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "900b23e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling with mean\n",
    "df.na.fill(mean_sales,subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5f83a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Doing it in single line\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebe40f",
   "metadata": {},
   "source": [
    "## Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4037d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('apple_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c8e1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0bb1be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ed29c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='2010-01-04', Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4ca6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofmonth,hour,dayofyear,month,year,weekofyear,format_number,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "359ddca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "|              11|\n",
      "|              12|\n",
      "|              13|\n",
      "|              14|\n",
      "|              15|\n",
      "|              19|\n",
      "|              20|\n",
      "|              21|\n",
      "|              22|\n",
      "|              25|\n",
      "|              26|\n",
      "|              27|\n",
      "|              28|\n",
      "|              29|\n",
      "|               1|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting day of month from the date column\n",
    "df.select(dayofmonth(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6a5caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          2|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Month extracted from Date column\n",
    "df.select(month(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35e4c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating new column (year)\n",
    "new_df = df.withColumn('Year',year(df['Date']))\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fea8bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2015|120.03999980555547|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2012| 576.0497195640002|\n",
      "|2016|104.60400786904763|\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Average closing price per year\n",
    "result = new_df.groupBy('Year').mean().select('Year','avg(Close)')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4561f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|2015|   120.03999980555547|\n",
      "|2013|    472.6348802857143|\n",
      "|2014|    295.4023416507935|\n",
      "|2012|    576.0497195640002|\n",
      "|2016|   104.60400786904763|\n",
      "|2010|    259.8424600000002|\n",
      "|2011|   364.00432532142867|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming column name\n",
    "new = result.withColumnRenamed('avg(Close)','Average Closing Price')\n",
    "new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e476aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|2015|               120.04|\n",
      "|2013|               472.63|\n",
      "|2014|               295.40|\n",
      "|2012|               576.05|\n",
      "|2016|               104.60|\n",
      "|2010|               259.84|\n",
      "|2011|               364.00|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Formatting number\n",
    "new.select(['Year',format_number('Average Closing Price',2).alias('Average Closing Price')]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
